# cs4243-assignment-3-solved
**TO GET THIS SOLUTION VISIT:** [CS4243 Assignment 3 Solved](https://www.ankitcodinghub.com/product/cs4243-instructions-solved-3/)


---

ğŸ“© **If you need this solution or have special requests:** **Email:** ankitcoding@gmail.com  
ğŸ“± **WhatsApp:** +1 419 877 7882  
ğŸ“„ **Get a quote instantly using this form:** [Ask Homework Questions](https://www.ankitcodinghub.com/services/ask-homework-questions/)

*We deliver fast, professional, and affordable academic help.*

---

<h2>Description</h2>



<div class="kk-star-ratings kksr-auto kksr-align-center kksr-valign-top" data-payload="{&quot;align&quot;:&quot;center&quot;,&quot;id&quot;:&quot;113954&quot;,&quot;slug&quot;:&quot;default&quot;,&quot;valign&quot;:&quot;top&quot;,&quot;ignore&quot;:&quot;&quot;,&quot;reference&quot;:&quot;auto&quot;,&quot;class&quot;:&quot;&quot;,&quot;count&quot;:&quot;1&quot;,&quot;legendonly&quot;:&quot;&quot;,&quot;readonly&quot;:&quot;&quot;,&quot;score&quot;:&quot;5&quot;,&quot;starsonly&quot;:&quot;&quot;,&quot;best&quot;:&quot;5&quot;,&quot;gap&quot;:&quot;4&quot;,&quot;greet&quot;:&quot;Rate this product&quot;,&quot;legend&quot;:&quot;5\/5 - (1 vote)&quot;,&quot;size&quot;:&quot;24&quot;,&quot;title&quot;:&quot;CS4243 Assignment 3 Solved&quot;,&quot;width&quot;:&quot;138&quot;,&quot;_legend&quot;:&quot;{score}\/{best} - ({count} {votes})&quot;,&quot;font_factor&quot;:&quot;1.25&quot;}">

<div class="kksr-stars">

<div class="kksr-stars-inactive">
            <div class="kksr-star" data-star="1" style="padding-right: 4px">


<div class="kksr-icon" style="width: 24px; height: 24px;"></div>
        </div>
            <div class="kksr-star" data-star="2" style="padding-right: 4px">


<div class="kksr-icon" style="width: 24px; height: 24px;"></div>
        </div>
            <div class="kksr-star" data-star="3" style="padding-right: 4px">


<div class="kksr-icon" style="width: 24px; height: 24px;"></div>
        </div>
            <div class="kksr-star" data-star="4" style="padding-right: 4px">


<div class="kksr-icon" style="width: 24px; height: 24px;"></div>
        </div>
            <div class="kksr-star" data-star="5" style="padding-right: 4px">


<div class="kksr-icon" style="width: 24px; height: 24px;"></div>
        </div>
    </div>

<div class="kksr-stars-active" style="width: 138px;">
            <div class="kksr-star" style="padding-right: 4px">


<div class="kksr-icon" style="width: 24px; height: 24px;"></div>
        </div>
            <div class="kksr-star" style="padding-right: 4px">


<div class="kksr-icon" style="width: 24px; height: 24px;"></div>
        </div>
            <div class="kksr-star" style="padding-right: 4px">


<div class="kksr-icon" style="width: 24px; height: 24px;"></div>
        </div>
            <div class="kksr-star" style="padding-right: 4px">


<div class="kksr-icon" style="width: 24px; height: 24px;"></div>
        </div>
            <div class="kksr-star" style="padding-right: 4px">


<div class="kksr-icon" style="width: 24px; height: 24px;"></div>
        </div>
    </div>
</div>


<div class="kksr-legend" style="font-size: 19.2px;">
            5/5 - (1 vote)    </div>
    </div>
This lab has four parts. For each part, implement the outlined functions in lab3.py and run them in the notebook lab3.ipynb. Exploratory questions are embedded directly at the end of the sections; write your answers directly into the notebook. Expected outputs are supplied as reference, but your outputs do not need to match exactly.

Upload to LumiNUS your completed lab3.py and lab3.ipynb by zipping them into a file named AXXX1_AXXX2.zip, where AXXX is the student number of the group members. Submit one file per group. Missing files, incorrectly formatted code that does not run, etc. will be penalized.

Objective

This lab covers keypoint detection and feature descriptors. We will explore the Harris Corner Detector, a simplified version of the SIFT descriptor and two applications for keypoint matching: image stitching and symmetry detection.

Unfortunately, we need to change the coordinate convention (x- and y-axis are swapped compared to Lab 2, see Figure on the right) to stay consistent with the conventions of the OpenCV functions which are used in this lab.

Part 1: Keypoint Detection, Description, and Matching (35%)

This part covers foundations of keypoint descriptors and matching which will be used in later parts. We provide the wrapper describe_keypoints which calls either the naÃ¯ve descriptor or the simplified version of SIFT.

â— harris_corners() detects Harris corner keypoints. For each pixel in an imsage,

1. Compute image gradient ğ¼ğ‘¥, ğ¼ğ‘¦ and compose the second moment matrix H. Note that the elements A, B, and C of matrix H are obtained by summing the elements in some window W:

i. ğ» = [ğ´ğµ ğµğ¶ ] ğ´ ğµ ğ‘Š ğ¼ğ‘¥ğ¼ğ‘¦ ğ¶

Consider an efficient implementation of the window-wise summation based on convolution (i.e., do not use for-loops). Weight the elements uniformly in the window for the summation. Hint: Consider using built-in functions skimage.filters.sobel_v, sobel_h and scipy.ndimage.filters.convolve.

2. Compute the corner response ğ‘… = ğ‘‘ğ‘’ğ‘¡(ğ»)â€“ ğ‘˜ (ğ‘¡ğ‘Ÿ(ğ»))2 with ğ‘˜ = 0.04.

3. Find local maxima using the provided non-maximum suppression; the local maxima in the response map corresponds to the keypoints.

â— naive_descriptor() computes a local descriptor based on the normalized pixel intensity,

ğ¼ ğœ‡

i.e. ğ¹(ğ‘¥, ğ‘¦) = . ğ¼(ğ‘¥, ğ‘¦) is the intensity at a keypoint located at (ğ‘¥, ğ‘¦) and ğœ‡ and ğœ are the

ğœ

mean and standard deviation of the pixel intensities in a patch of 5Ã—5 centered at (ğ‘¥, ğ‘¦). The 0.0001 term in the numerator improves numerical stability.

â— simple_sift() is a simplified version of the full SIFT descriptor which is not rotationally invariant. For each keypoint, take a 16 Ã— 16 patch of pixels around the keypoint and compute the image gradient magnitude and orientation at each pixel. For efficiency, consider precomputing the image gradients for the entire image in advance. Divide the patch into a 4Ã—4 grid of cells, where each cell has 4 Ã— 4 = 16 pixels (see Figure 1a).

Figure 1. Schematic visualization of estimating our simplified SIFT descriptor.

For each cell, create a histogram of the gradient orientations with 8 bins of 45Â° each (e.g., 0Â° to 44Â° for bin 0, 45Â° to 89Â° for bin 1, etc.). Each pixelâ€™s contribution to the histogram is weighted by the pixelâ€™s gradient magnitude and the weight from a Gaussian kernel of 16 Ã— 16 centered on the keypoint coordinate. Use the provided make_gaussian_kernel. Concatenate the histograms from each cell to form a 4Ã—4Ã—8=128-dimension vector (see Figure 1b). Normalize the vector to unit length by dividing by the vectorâ€™s magnitude [link].

â— top_k_matches()finds the k top matches between two feature descriptor sets ğ¹1 =

{ğ¹11,ğ¹12 â€¦ ğ¹1ğ‘– â€¦ ğ¹1ğ‘€} and ğ¹2 = {ğ¹21,ğ¹22 â€¦ ğ¹2ğ‘— â€¦ ğ¹2ğ‘}. For each descriptor in ğ¹1, find the ğ‘˜ descriptors in ğ¹2 with the smallest Euclidean distance. Give the output as a list of length-2 tuples. For each tuple, the first element is the descriptor index ğ‘– from ğ¹1 and the second element is a ğ‘˜long list of length-2 tuples, with indices ğ‘— in ğ¹2 and the Euclidean distance ğ·(ğ¹1ğ‘–, ğ¹2ğ‘—). Hint: consider using scipy.spatial.distance.cdist().

â— ratio_test_match() checks for ğ¹1ğ‘– the ratio between the top 2 matches ğ¹2ğ‘ and ğ¹2ğ‘. If

ğ·

&lt; ğœ, then ğ¹1ğ‘– and ğ¹2ğ‘ are a match. The ratio threshold ğœ is given as an argument.

ğ·(ğ¹1ğ‘–,ğ¹2ğ‘)

Part 2: Image Stitching (25%)

This part uses the matched descriptor pairs from Part 1 and solves the homography between the two images. The images are then warped and stitched together. For this part, use your SIFT descriptor implementation (simple_sift) to match the keypoints for image fp1.png and fp2.png.

If you need more details to help you with the implementation, refer to this StackOverflow post.

â— For implementing the above two functions, use the provided transform_homography function that applies a given homography to a set of points.

Part 3: Mirror Symmetry Detection (25%)

For Parts 3 and 4, use the provided function compute_cv2_descriptor, which uses OpenCVâ€™s SIFT implementation since we require rotation-invariant descriptors. Mirror symmetry is defined by a line of reflection or symmetry line (dashed red line in Fig. 3b). This part performs SIFT keypoint matching to solve for candidate points on the symmetry line. SIFT descriptors are rotationally invariant, but not mirror-invariant, so how can they be matched? We create a virtual set of â€œmirrorâ€ descriptors by re-assigning the cell indices and histogram bin indices.

â— create_mirror_descriptors()using the provided compute_cv2_descriptor, detect keypoints and compute descriptors from the original image ğ¼ğ‘œ denoted as ğ¹ğ‘œ =

{ğ¹ğ‘‚1,ğ¹ğ‘‚2 â€¦ ğ¹ğ‘‚ğ‘– â€¦ ğ¹ğ‘‚ğ‘€}.

â— shift_sift_descriptor() creates a virtual set of mirror descriptors (see Fig. 3b). Treat the image as if it has been mirrored over a virtual vertical axis (see Figure 3a for conceptual visualization). Shift the cell indices and bin indices of each 8-dimensional vector accordingly. For each 8-dim. vector, as SIFT already shifts the indices to keep the dominant orientation first, the first index (0) will stay in the same position. Remaining bins are reversed, i.e. [0, 1, 2, ..7] remaps to [0, 7, 6, .., 2, 1]. See lab3.py for an example of a SIFT histogram vs. its mirrored version.

â— match_mirror_descriptors()matches keypoints between ğ¹ğ‘œ and mirrored descriptors

ğ¹ğ‘‚â€² = {ğ¹ğ‘‚â€²1,ğ¹ğ‘‚â€²2 â€¦ ğ¹ğ‘‚â€²ğ‘– â€¦ ğ¹ğ‘‚â€²ğ‘€} (see Fig. 3c)

Figure 3. Symmetric feature matching.

â— find_symmetry_lines()takes in pairs of matched descriptors and votes for a candidate symmetry line via the Hough transform. For a matched pair of points ğ‘– and ğ‘˜â€² (see Fig 3d), form a line that intersects the two points; this line is perpendicular to the symmetry line. Solve for the midpoint ğ‘š = (ğ‘¥ğ‘š, ğ‘¦ğ‘š) between ğ‘– and ğ‘˜â€² on the intersecting line and compute the angle ğœƒğ‘š that the line makes with the x-axis.

â— hough_vote_mirror() performs Hough voting with parameter space (ğœŒ,Î¸). Each intersecting point ğ‘š has one vote defined by ğœƒğ‘š and ğœŒğ‘š = ğ‘¥ğ‘š Ã— ğ‘ğ‘œğ‘  (ğœƒğ‘š) + ğ‘¦ğ‘š Ã— ğ‘ ğ‘–ğ‘› (ğœƒğ‘š) . The line(s) of symmetry in the image will be represented by local maxima in the Hough vote space. Use the function find_peak_params (from Lab 2); num_lines is a parameter which limits the number of local maxima that are returned.

Part 4: Rotation Symmetry Detection (15%)

Keypoint matching and Hough voting can also be used to detect rotational symmetry. For this part, we use the orientation and scale attributes for the cv2.Keypoint class named angle and size.

Figure 5. Rotational symmetry detection: a) A shape looks the same after rotations; b) A pair of matched keypoints (green) vote for one candidate center of rotation (red); c) Hough voting is used to select a maxima point from the candidate centers as the predicted center of rotation.

â— Using compute_cv2_descriptor, detect keypoints and compute descriptors from a given image denoted as ğ¹ = {ğ¹1,ğ¹2 â€¦ ğ¹ğ‘– â€¦ ğ¹ğ‘€}.

â— Then implement the function match_with_self which uses top_k_matches, giving ğ¹ as both ğ¹1 and ğ¹2 since we are matching descriptors on an image to itself to find the best three matches. Eliminate the trivial match (a keypoint is matched with itself) and perform the ratio test on the other two matches. If no match is eliminated, pick the best two.

â— find_rotation_centers()estimates, for a matched pair of keypoints ğ‘ğ‘– = (ğ‘¥ğ‘–, ğ‘¦ğ‘–) and ğ‘ğ‘— = (ğ‘¥ğ‘—, ğ‘¦ğ‘—), the coordinates for a candidate centre of rotation:

a. Remove â€œparallelâ€ matches based on the keypoint orientation i.e. the angle. Wrap around the orientations ğ›·ğ‘– and ğ›·ğ‘— for ğ‘ğ‘– and ğ‘ğ‘— respectively to ensure they are within [0, 2ğœ‹). If ğ›·ğ‘– and ğ›·ğ‘— are within 1 degree of each other, discard ğ‘ğ‘– and ğ‘ğ‘—. The similar orientations suggest â€œparallelâ€ keypoints, i.e. there exists no centre of rotation about which ğ‘ğ‘– can be rotated to coincide with ğ‘ğ‘—.

b. Form a line of intersection (see red line in Fig. 4) between ğ‘ğ‘– and ğ‘ğ‘— and solve for this line the length ğ‘‘ and the angle ğ›¾ that it forms with the x-axis.

c. A center of rotation ğ‘ğ‘–ğ‘— = (ğ‘¥ğ‘, ğ‘¦ğ‘) between

ğ‘ğ‘– and ğ‘ğ‘— can be defined by angle

ğ›½ = and radius ğ‘Ÿ with coordinates

ğ‘¥ğ‘ = ğ‘¥ğ‘– + ğ‘Ÿ Ã— ğ‘ğ‘œğ‘ (ğ›½ + ğ›¾) and ğ‘¦ğ‘ = ğ‘¦ğ‘– + ğ‘Ÿ Ã— ğ‘ ğ‘–ğ‘›(ğ›½ + ğ›¾). Discard if the center is out of image bounds.

â— hough_vote_rotation()does Hough voting on the rotation coordinates with

parameter space (ğ‘¥, ğ‘¦). Matched keypoints ğ‘ğ‘– and ğ‘ğ‘— vote for the associated center location (ğ‘¥ğ‘, ğ‘¦ğ‘) with the vote weighted by the keypoint scales ğ‘ ğ‘– and ğ‘ ğ‘— respectively. The vote weight is defined as ğ‘¤ = (ğ‘’ğ‘)2 where ğ‘ = âˆ’ |ğ‘ ğ‘–âˆ’ğ‘ ğ‘—|. The global maxima should

(ğ‘ ğ‘–+ğ‘ ğ‘—)

correspond to the center of rotation.

â— Note: pay attention to our coordinate choice because to get the point (ğ‘¥ğ‘–, ğ‘¦ğ‘–) you will need to call I[yi][xi] in your code.
